---
title: "Gaussian Mixture Models"
author: "Anna Kubasiak"
date: "13 March 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(mcsm)
library(mvtnorm)
library(ellipse)
library(ggplot2)
# library(ggExtra)
# library(knitr)
```

#*To do*

Motywacja dlaczego mixture models
- Kiedy regresja nie jest dobrym podejsciem  
- mechaniczne podejscie do regresji liniowej moze dac zupelnie bledne wyniki
- przyklad
- jak zrobic to lepiej
- dlaczego mieszanki*

```{r motivation, echo=FALSE, message=FALSE, fig.height=3, fig.width=6 }
 example<-mvrnorm(10000,mu=c(0,0),Sigma=matrix(c(1,.9,.9,1),ncol=2)) %>% as.data.frame
names(example)<-c('x','y')

 linearModel<-lm('y~x',data=example)
 a<-linearModel$coefficients[[2]] %>% round(3)
 b<-linearModel$coefficients[[1]] %>% round(3)
 if(b>0){lmLabel<-paste0("y=",a,"*x+",b)}else(lmLabel<-paste0("y=",a,"*x",b))
 examplePoints<-data.frame(x=round(min(example[,1])):round(max(example[,1])))
 lmPredicted<-predict(linearModel, examplePoints, se.fit = TRUE)$fit
 labelPredicted<-paste0("(",examplePoints$x,",",round(lmPredicted,2),")")
  p<-ggplot()+theme_bw()+geom_point(data=example,aes(x=x,y=y), alpha=0.05, color=hsv(0.60))
 p
```
First thing coming to mind is to fit a linear regression to those points. This way we can find the most probable $y$ given any $x$:

```{r motivation2, echo=FALSE, message=FALSE, fig.height=3, fig.width=6 }
 p+ geom_abline(slope=a,intercept = b)+
   geom_label(aes(x=mean(example[,1]),y=min(example[2])),label=lmLabel,cex=3)+
   geom_point(aes(x=examplePoints,y=lmPredicted))+
   geom_label(aes(x=examplePoints,y=lmPredicted),label=labelPredicted,nudge_x=0.8,cex=3)
 
```

What is really going on behind the scene is following:
To calculate a value of $y$ given a $x$ we calculate **conditional** probability $p(y|x)$ based on the whole joint $p(x,y)$. Namely: if we cut our cloud of points in any section $x=x_0$ we will find some probability density of $y|x_0$'s where $E(y|x_0)$ is in fact approximated with the linear regression value $y(x_0)$:

```{r motivation3, echo=FALSE, message=FALSE, fig.height=3, fig.width=6}

myPoint<-examplePoints[6,1]
dataX<-example[abs(example$x-myPoint)<0.1,1]
dataY<-example[abs(example$x-myPoint)<0.1,2]

dens<-cbind(density(dataY)$y+myPoint,density(dataY)$x)
h<-mean(dataY)
smallLine<-data.frame(x=c(min(dens[,1]),max(dens[,1])),y=rep(h,nrow(dens)))
conditionalLablel<-paste0("(",myPoint,",",round(h,2),")")

 p<-ggplot()+theme_bw()+geom_point(data=example,aes(x=x,y=y), alpha=0.05, color=hsv(0.60))+
geom_point(aes(x=examplePoints,y=lmPredicted))+
geom_vline(xintercept=myPoint)+ geom_path(aes(x=dens[,1],y=dens[,2]))+ geom_path(aes(x=smallLine[,1],y=smallLine[,2]),linetype=3)+
   geom_label(aes(x=myPoint+max(smallLine[,1]),h),label=conditionalLablel)
# p
myPoint<-examplePoints[3,1]
dataX<-example[abs(example$x-myPoint)<0.1,1]
dataY<-example[abs(example$x-myPoint)<0.1,2]

dens2<-cbind(density(dataY)$y+myPoint,density(dataY)$x)
h2<-mean(dataY)
smallLine2<-data.frame(x=c(min(dens2[,1]),max(dens2[,1])),y=rep(h2,nrow(dens2)))
conditionalLablel2<-paste0("(",myPoint,",",round(h2,2),")")

 p<-p+
geom_vline(xintercept=myPoint)+ geom_path(aes(x=dens2[,1],y=dens2[,2]))+ geom_path(aes(x=smallLine2[,1],y=smallLine2[,2]),linetype=3)+
   geom_label(aes(x=myPoint+(max(smallLine2[,1])),h2),label=conditionalLablel2)
p

```

#The rest

This is the first chapter of Mixture Model Tutorial. In this chapter we will start with the simplest case of Gaussian Mixture Model where the average $\mu$ and covariance matrix $\Sigma$ of each component model is known as well as the proportion of each component $\pi$ and the number of components $K$. Our goal in this chapter will be to find out for a set of measurements {${x_i}$}$_{i=1..N}$ from which component distribution those measurement points come from: $p(x_i)$.


```{r generate distributions, echo=FALSE, message = FALSE}
set.seed(13)

#Number of distributions
K=5
d=2
s<-0.6
nSim<-1000
maxMu<-6
# parameters of each distribution
mu<-list()
sigma<-list()
ellip<-list()
for (i in 1:K){
  mu[[i]]<-sample(1:maxMu,d)
  mat1<-matrix(rnorm(d^2,0,s),ncol=d)
  sigma[[i]]<-mat1%*%t(mat1) %>% round(2)
  ellip[[i]]<-ellipse(sigma[[i]],centre=mu[[i]])
}
# color for each distribution
distColor<-data.frame(hsv=hsv((1:K)/K))
distColor$distR=distColor$hsv %>% substr(2,3) %>% as.hexmode %>% as.integer
distColor$distG=distColor$hsv %>% substr(4,5)%>% as.hexmode %>% as.integer
distColor$distB=distColor$hsv %>% substr(6,7)%>% as.hexmode %>% as.integer


# prior probabilities 
temp<-runif(K,min=0,max=1)
pi<-round(temp/sum(temp),2) 
muDf<-as.data.frame(mu)
names(muDf)<-paste0("mu",1:K)

```

## Gaussian mixture with known parapeters


```{r,fig.width=3, fig.height=2.8, distributions, message=FALSE, echo=FALSE}

p<-ggplot()+ 
  theme_bw()+
  ggtitle(label='Contours of 2d gaussians')


for (k in 1:K){
  
  muLabel<-paste(expression(mu),"=","(",(paste(mu[[k]],collapse=',')),")",sep='')
  p<-p+
    geom_path(data=as.data.frame(ellip[[k]]),aes(x,y),color=distColor$hsv[k],lwd=1.5)
  # geom_point(aes(x=mu[[k]][1],y=mu[[k]][2]),lwd=2,color=distColor$hsv[k])
}
p+geom_point(aes(x=as.numeric(muDf[1,]),y=as.numeric(muDf[2,])),lwd=2,color=distColor$hsv)+
  geom_label(aes(x=as.numeric(muDf[1,]),y=as.numeric(muDf[2,])),label=1:K,color=as.character(distColor$hsv),nudge_x=0.5)


dataPi=data.frame(distribution=1:length(pi),pi=pi)
ggplot(data=dataPi,aes(x=distribution,y=pi))+
  geom_bar(fill=distColor$hsv,stat='identity')+
  theme_bw()+
  ggtitle("Distributions proportions")
# p+geom_point(data=allX, aes(x=d1,y=d2),alpha=0.5,size=0.9)
# 
# 
# ggplot(allX, aes(x = d1, y = d2)) + geom_point() +geom_density2d()+
#     geom_path(data=as.data.frame(ellip[[k]]),aes(x,y),color=distColor$hsv[k])
```


In this example we will work with 2D gaussian distributions. We set K=`r I(K)`.
Then we simulate a measurment by choosing N=nSim=`r nSim` points in proportion $\pi$=(`r pi`).

```{r simulations, fig.width=3, fig.height=2.8,message=FALSE,echo=FALSE}
# simulating
x<-list()
allX<-NULL
idx<-0
xlimits<-c(0,0)
for(i in  1:K)
{
  sampleSize<-round(nSim*pi[i])
  # x[[i]]<-mvrnorm(n=sampleSize,mu=mu[[i]],Sigma=sigma[[i]])
  x[[i]]<-rmvnorm(n=sampleSize,mean=mu[[i]],sigma=sigma[[i]])
  
  allX<-rbind(allX,x[[i]])
}
if(d==2) {colnames(allX)<-c('x','y')}else {colnames(allX)<-paste0("d",1:d)}

ggplot(data=data.frame(allX), aes(x = x, y = y)) + geom_point(size=1, alpha=1,color='black') +geom_density2d(lwd=0.7)+theme_bw()

```

## Now what do we do?

We would like to formulate the problem in terms of a function maximalization where we could use the knowledge of measurements and the parameters. 
First we can notice that the joint probability distribution of X is complicated since it is weighted average of $K$ component distributoins. On the other hand each of the components is simple. It gives us a hint that operating on **conditional** probabilities is going to be easier. Calculating probabilities on the condition that we know which component is responsible for a measurment is straightforward. 

## The latent variable 

To write down the conditional probabilities we need to define a variable that tells us which condition is fulfilled. This variable will be called $z$ and it will be $K$-component vector defined for each measurment with all but one components equal to $0$ with the exception being equal to $1$: 

\begin{align*}
z_n\equiv(z_{n1},..,z_{nK}):\\
\{z_{nk} \}_{k=1..K}\in \{0,1\},\\
\sum_{k=1}^K z_{nk}=1
\end{align*}

The interpretation of this variable is as follows: $z_{n k}=1$ tells us that $n$-th measurment belongs to $k$-th component distribution. And since $\sum_{k=1..K}z_{nk}=1$, it belongs only to one component. 



We know that the proportoins of each components are $\pi_k$. Using the new variable $z_{n}$ we can write the probability that $n$-th measurment belongs to any component as:

\begin{align*}
p(z_n)=
\Pi_{k=1}^K \pi_k^{z_{nk}}
\end{align*}
Also 

\begin{align*}
p(x_n|z_n)=\Pi_{k=1}^K \mathcal{N}(x_n|\mu_k,\Sigma_k)^{z_{nk}}
\end{align*}

And so we can write down $p(x_n)$ in terms of conditional probabilities:

\begin{align*}
p(x_n)= \sum_{z_n} p(z_n) p(x_n|z_n)=\\
\sum_{z_n} \Pi_{k=1}^K \pi_k^{z_{nk}}\mathcal{N}(x_n|\mu_k,\Sigma_k)^{z_{nk}}=\\
\sum_{k=1}^K \pi_k \mathcal{N}(x_n|\mu_k,\Sigma_k)
\end{align*}


We arrived at a very intuitive formula and the formulation that led us to it may seem excessively complicated at this point, but it allowes us to write down a closed form probabilities that take into account measurment's associatiation with a component distributon and will be very useful. 

## Bayes theorem

Now we can use the Bayes theorem to inverse our reasoning and find out the chances that each of our measurment points belong to each component distribution. The theorem looks innocent enough:

\begin{align*}
p(A|B)p(B)=p(B|A)p(A)
\end{align*}

It allowes to swap the condition with variable. Using it we can write:

\begin{align*}
p(z_n|x_n)p(x_n)=p(z_n)p(x_n|z_n)
\end{align*}
So if we choose to test any realization of $z_n$, lets say with $k-th$ component nonzero which is equivalent to asking a question: "Does my point $x_n$ belong to $k$-th component distribution?" we can write:

\begin{align*}
p(z_n: z_{nk}=1|x_n)&=&\\
\frac{p(z_{n})p(x_n|z_n)}{p(x_n)}&=&\\
\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_n|\mu_j,\Sigma_j)}&\equiv \gamma(z_{nk})&
\end{align*}

After applying this formula to our measurement data set we can estimate to which of $K$ gaussian component distributions each of them belong. 

```{r posterior probabilities,echo=FALSE,message=FALSE}
#  calculating posterior for each simulated point in allX
normalisation<-rep(0,nrow(allX))
for(i in 1:K){
  normalisation<- normalisation + pi[i]*dmvnorm(allX[,1:2],mean=mu[[i]],sigma=sigma[[i]])
}
for (i in 1:K){
  gamma<-pi[i]*dmvnorm(allX[,1:2],mean=mu[[i]],sigma=sigma[[i]])/normalisation
  
  allX<-cbind(allX,gamma)
  
}
colnames(allX)[3:(K+2)]<-paste0("gamma",1:K)
```


```{r plot estimations1, fig.width=3,fig.height=2.8,echo=FALSE,message=FALSE}
# defining colors of each point based on gammas
allXColors<-as.matrix(allX[,3:(K+2)])%*%as.matrix(distColor[,2:4]) %>% round() %>% as.data.frame
allXColors$color<-paste0("#",as.hexmode(allXColors$distR),as.hexmode(allXColors$distG),as.hexmode(allXColors$distB))
allX<-cbind(allX,allXColors)


# plotting real distributions
p<-ggplot(data=allX,aes(x,y))+geom_point(alpha=1/4,color=allX$color,size=1.5)+ 
  theme_bw()
p  
for (k in 1:K){
  p<-p+geom_path(data=as.data.frame(ellip[[k]]),aes(x,y),color=distColor$hsv[k])
}

p
```

## How well did we guess?
We can fit the gaussian distributions to the points according to our guess which component they belong to and see if mean and variance are a good match of the real ones. 
One way to do it is to associate each point with one distribution only by choosing the one which has the highest probability. 
Alternatively we can use weighted average and weighted covariance matrix to use all data points estimating each of our $K$ component distributions. Results of those two options are plotted below.

```{r fitting1, echo=FALSE,message=FALSE,fig.width=3,fig.height=2.8}

allX$bestFittingComponent<-apply(allX[,3:(2+K)],1,function(x){
  m<-max(x)
  which(x==m)
})

fittedMu<-list()
fittedSigma<-list()
fittedEllip<-list()
p<-ggplot()+theme_bw()+ggtitle('Most probable component')
for (k in 1:K){
  componentData<-allX[allX$bestFittingComponent==k,]
  fittedMu[[k]]<-apply(componentData[,1:2],2,mean)
  fittedSigma[[k]]<-componentData[,1:2] %>% var()
  fittedEllip[[k]]<-ellipse(fittedSigma[[k]],centre=fittedMu[[k]])
  
  
  p<-p+
    geom_path(data=as.data.frame(ellip[[k]]),aes(x,y),color=distColor$hsv[k])+
    geom_path(data=as.data.frame(fittedEllip[[k]]),aes(x,y),color=distColor$hsv[k],linetype=2)
}
fittedMu<-as.data.frame(fittedMu)
names(fittedMu)<-paste0("mu",1:K,"fitted")

p+geom_point(aes(x=as.numeric(fittedMu[1,]),y=as.numeric(fittedMu[2,])),color=distColor$hsv,shape=21)+
  geom_point(aes(x=as.numeric(muDf[1,]),y=as.numeric(muDf[2,])),color=distColor$hsv)

```
```{r fitting2, echo=FALSE,message=FALSE,fig.width=3,fig.height=2.8}
fittedMuWt<-list()
fittedSigmaWt<-list()
fittedEllipWt<-list()
p<-ggplot()+theme_bw()+ggtitle('Weighted estimators')
for (k in 1:K){
  estimators<-cov.wt(allX[,1:2], wt = allX[,2+k])
  fittedMuWt[[k]]<-estimators$center
  fittedSigmaWt[[k]]<-estimators$cov
  fittedEllipWt[[k]]<-ellipse(fittedSigmaWt[[k]],centre=fittedMuWt[[k]])
  
  
  p<-p+
    geom_path(data=as.data.frame(ellip[[k]]),aes(x,y),color=distColor$hsv[k])+
    geom_path(data=as.data.frame(fittedEllipWt[[k]]),aes(x,y),color=distColor$hsv[k],linetype=2)
}
fittedMuWt<-as.data.frame(fittedMuWt)
names(fittedMuWt)<-paste0("mu",1:K,"fitted")

p+geom_point(aes(x=as.numeric(fittedMuWt[1,]),y=as.numeric(fittedMuWt[2,])),color=distColor$hsv,shape=21)+
  geom_point(aes(x=as.numeric(muDf[1,]),y=as.numeric(muDf[2,])),color=distColor$hsv)

```


